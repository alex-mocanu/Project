{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project milestone 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This last notebook presents the final results of our work. After producing a detailed analysis of the different datasets, we focused our analysis on three datasets to produce the most optimal analysis and presentation of results.\n",
    "These datasets are:\n",
    "* Stack Overflow\n",
    "* Wikipedia\n",
    "* [Crowdedness at the Gym Campus](https://www.kaggle.com/nsrose7224/crowdedness-at-the-campus-gym/version/2)\n",
    "\n",
    "\n",
    "These three datasets allow us to find the same trends that we will present below.\n",
    "A report is also available here offering a more detailed and organized analysis.\n",
    "Hoping that you will enjoy our work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import seaborn as sns\n",
    "from ipywidgets import interact\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "%matplotlib inline\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import min\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "\n",
    "from pandas.plotting import scatter_matrix\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daylight Saving Time impact on Stack Overflow posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generated a dataset based on the Wikipedia data to complete our analysis with a validation with another data source.\n",
    "To obtain this data, we wrote a simple Python script, which makes use of the Wikipedia API (https://www.mediawiki.org/wiki/API:Recent_changes_stream).\n",
    "\n",
    "Wikipedia only allow their users to get the last 30 days changes. Relative to the moment we finished writing the script, we chose a period of 31 days, from 2018-10-18 13:38:21 to 2018-11-17 00:00:15, in other words, 17 days before the hour change and 13 days after.\n",
    "\n",
    "This time window is relevant for our problem, because we need to analyze a long-period behavior evolution of users.\n",
    "\n",
    "The number of changes per second in Wikipedia is large. That's why we had to limit the number of changes we got (31 days of changes represent  approximately 560 GB of data).\n",
    "\n",
    "We got the 100th first changes each 15 minutes for our time period to limit the amount of data to be stored.\n",
    "\n",
    "Because of this choice, we can't compare the number of changes before and after the hour change. We have to find others indicators to validate our theories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wikipedia changes API request format\n",
    "\n",
    "A sample API request looks like this:\n",
    "https://en.wikipedia.org/w/api.php?action=query&format=json&list=recentchanges&rcprop=ids%7Csizes%7Cfrcshow=!bot%7Clags%7Cuserid%7Ctimestamp&rclimit=100&rcstart='+wikistamp+'&rcdir=newer\n",
    "\n",
    "The parameters used for the API requests have the following role:\n",
    "- rcprop: which parameters to keep from the API response (here: id, size, exclude bot changes, userid, timestamp)\n",
    "- rclimit: how many changes to get for each request (here: 100 per request)\n",
    "- rcstart: the timestamp to start listing from (here: we put an iterator and add 15 minutes after each iteration)\n",
    "- rcdir: direction to list in (newer/older)\n",
    "- wikistamp: Python variable to increment timestamp call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Analysis\n",
    "\n",
    "Having collected the dataset, we can now begin to analyse it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the wikipedia dataset\n",
    "DATA = './data/wikiData.json'\n",
    "wiki_df = spark.read.json(DATA)\n",
    "wiki_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data in parquet format for faster loading afterwards\n",
    "wiki_df.write.parquet(\"posts.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given the reasonable size of the data, transform it into a Pandas dataframe\n",
    "df = wiki_df.toPandas()\n",
    "df['timestamp'] =  pd.to_datetime(df['timestamp'], format='%Y-%m-%d %H:%M:%S')\n",
    "df = df.sort_values(by=['timestamp'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can delete the *actionhidden* (title is hidden or not) and *anon* (user is logged in or not) parameters. We also keep *userid* and remove *userhidden* (user is hidden or not), because if the userid is 0, we already knoe the user is hidden. The *suppressed* parameter can be deleted too, because it's just a boolean which is true when more details about the suppression is available and it's not useful for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['anon','actionhidden','suppressed','userhidden'],axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now assign a numerical value to the different string values contained in type, in order to be able to correlate them with the other values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.type.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To refer to wikipedia API : \n",
    "- edit: Regular page edits\n",
    "- new: Page creations (Uploads are not listed as new but as log)\n",
    "- log: Log entries\n",
    "- categorize: Page categorizations\n",
    "\n",
    "We will use the convention:\n",
    "- edit = 0\n",
    "- categorize = 1\n",
    "- new = 2\n",
    "- log = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map type values to ids\n",
    "type_id = {'edit': 0, 'categorize': 1, 'new': 2, 'log': 3}\n",
    "\n",
    "df.type = df.type.map(lambda x: type_id[x])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find the number of changes with newlen and oldlen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lenchanged'] = df['newlen'] - df['oldlen']\n",
    "df = df.drop(['newlen','oldlen'],axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display fist and last entries' dates\n",
    "print(f'First change dates back to {df.timestamp.values[0]}')\n",
    "print(f'Last change dates back to {df.timestamp.values[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataframe in 'before' and 'after' hour change\n",
    "change_hour = datetime.utcfromtimestamp(1541293200).strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "before_df = df.loc[df['timestamp'] < change_hour]\n",
    "after_df = df.loc[df['timestamp'] > change_hour]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation\n",
    "\n",
    "Let's take a look at the correlation between the data fields that we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(10, 8))\n",
    "corr = df.corr()\n",
    "_ = sns.heatmap(corr, mask=np.zeros_like(corr, dtype=np.bool), cmap=sns.diverging_palette(220, 10, as_cmap=True),\n",
    "            square=True, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix shows us a correlation between old and new revision ids, explained by the fact the two numbers are always of the same order of magnitude.\n",
    "\n",
    "Now, let's take a look at how the types of changes are distributed before and after the hour change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of the change type before the hour change\n",
    "id_to_type = {0: 'edit', 1: 'categorize', 2: 'new', 3: 'log'}\n",
    "before_categories_df = before_df.groupby('type').size().sort_values(ascending=False).to_frame('count')\n",
    "before_categories_df.reset_index(level=0, inplace=True)\n",
    "before_categories_df.type = before_categories_df.type.map(lambda x: id_to_type[x])\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "sns.barplot(x='count', y='type', data=before_categories_df).set_title('Changes distribution before hour change')\n",
    "display(before_categories_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of the change type after the hour change\n",
    "after_categories_df = after_df.groupby('type').size().sort_values(ascending=False).to_frame('count')\n",
    "after_categories_df.reset_index(level=0, inplace=True)\n",
    "after_categories_df.type = after_categories_df.type.map(lambda x: id_to_type[x])\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "sns.barplot(x='count', y='type', data=after_categories_df).set_title('Changes distribution before hour change')\n",
    "display(after_categories_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the proportions between the counts before and after the hour change for each category\n",
    "proportions = after_categories_df['count'] / before_categories_df['count']\n",
    "prop_df = pd.DataFrame({'type': ['edit', 'categorize', 'log', 'new'], 'proportion': proportions})\n",
    "display(prop_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of each category seem similar, the difference in numbers being due to the fact that there are more days before than after DST. We could just note that there seems to be a slightly bigger proportion of logs and a smaller proportion of new articles, but the differences are not numerically considerable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of Lines\n",
    "\n",
    "Let's see if there's some modification into the number of lines changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider the absolute value in the number of lines changed\n",
    "edit_before = before_df.loc[before_df['type'] == 0].copy()\n",
    "edit_before.loc[:, 'lenchanged'] = edit_before.lenchanged.abs()\n",
    "\n",
    "print('Line change statistics before hour change')\n",
    "edit_before.lenchanged.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb = edit_before['timestamp']\n",
    "yb = edit_before['lenchanged']\n",
    "fig = plt.figure(figsize=(20, 5))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(xb, yb)\n",
    "plt.title('Line change before hour change')\n",
    "plt.xlabel('Time')\n",
    "_ = plt.ylabel('Line changes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider the absolute value in the number of lines changed\n",
    "edit_after = after_df.loc[after_df['type'] == 0].copy()\n",
    "edit_after.lenchanged = edit_after.lenchanged.abs()\n",
    "\n",
    "print('Line change statistics after hour change')\n",
    "edit_after.lenchanged.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xa = edit_after['timestamp']\n",
    "ya = edit_after['lenchanged']\n",
    "fig = plt.figure(figsize=(20, 5))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(xa, ya)\n",
    "plt.title('Line change after hour change')\n",
    "plt.xlabel('Time')\n",
    "_ = plt.ylabel('Line changes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysing visually, we can see that number of peaks in the edits done after the hour change is higher than for the period before the hour change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "#### Data Handle\n",
    "The dataset that we collected is quite small (53MB), therefore we don't need to make use of Spark in order to handle it.\n",
    "\n",
    "In what comes next, we plan to collect more data, so that we can better understand the behaviour evolution before and after the hour change.\n",
    "\n",
    "#### Understanding the Data\n",
    "After cleaning and formatting our data to achieve the best possible interpretation, we began to analyze it.\n",
    "Despite some correlation with no real causality, we have been able to better understand which analysis we should focus on: the dataset is relatively small and the features obtained do not carry a lot of information, we will focus on the activity of people through the amount of data modified before and after the time change.\n",
    "\n",
    "#### Transform the Data\n",
    "To improve the accuracy of our work, users' data can be cross-referenced to get their geographic coordinates and get a more accurate idea of the effects of time changes on specific geographic areas.\n",
    "\n",
    "#### Methodology\n",
    "Getting a complete analysis with only this dataset is impossible considering its size. This is why it will serve as a validation set for the analysis that we will perform in the data that we are going to collect.\n",
    "We will therefore split this dataset in a short time interval in order to perform an analysis of the activity on the type of changes of the articles and on the amount of information modified, around the date of the hour change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Larger dataset follow-up\n",
    "\n",
    "In what comes next, we decided to collect a larger dataset comprising all the changes done in the week before and the week after the winter hour change for this year. For this, we once again used the Wikipedia API described above to collect a total of 595 MB of data. Due to the size of the new dataset, we will make use of Spark this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "EXTENDED_DATA = \"./data/wikiExtendedData.json\"\n",
    "wiki_df = spark.read.json(EXTENDED_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the data format\n",
    "wiki_df.printSchema()\n",
    "wiki_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register view to do SQL queries\n",
    "wiki_df.createOrReplaceTempView(\"Wiki\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_date = spark.sql(f\"\"\"SELECT COUNT(*) AS PostsCount,\n",
    "            DATE_FORMAT(timestamp, \"yyyy-MM-dd'T'HH\") as Date\n",
    "            FROM Wiki\n",
    "            GROUP BY Date\n",
    "            ORDER BY Date ASC\"\"\")\n",
    "\n",
    "posts_date.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_date_df = posts_date.toPandas()\n",
    "posts_date_df['Hour'] = posts_date_df['Date'].apply(lambda x: x.split('T')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data in before and after the hour change\n",
    "before_df = posts_date_df[:167]\n",
    "after_df = posts_date_df[167:]\n",
    "\n",
    "\n",
    "before_df = before_df.groupby('Hour').mean()\n",
    "after_df = after_df.groupby('Hour').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the count distributions\n",
    "f, arr = plt.subplots(1, 2, sharey=True)\n",
    "f.suptitle(f'Change counts before and after winter hour change 2018')\n",
    "f.set_figwidth(12)\n",
    "f.set_figheight(6)\n",
    "before_df.groupby('Hour').mean().reset_index().plot.bar(ax=arr[0], x='Hour', y='PostsCount',\n",
    "                                                        title='Before', color='blue')\n",
    "_ = after_df.groupby('Hour').mean().reset_index().plot.bar(ax=arr[1], x='Hour', y='PostsCount',\n",
    "                                                           title='After', color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, arr = plt.subplots(1, 2, sharey=True)\n",
    "f.suptitle(f'Change counts before and after winter hour change 2018')\n",
    "f.set_figwidth(12)\n",
    "f.set_figheight(6)\n",
    "plt.title(\"Before\")\n",
    "before_df.boxplot(ax=arr[0],column=['PostsCount'])\n",
    "plt.title(\"After\")\n",
    "boxplot2 = after_df.boxplot(ax=arr[1],column=['PostsCount'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the count distributions\n",
    "before = before_df.groupby('Hour').mean().reset_index()\n",
    "after=after_df.groupby('Hour').mean().reset_index()\n",
    "before_display = before.rename(columns={'PostsCount':'before'})\n",
    "after_display = after.rename(columns={'PostsCount':'after'})\n",
    "f, arr = plt.subplots(1, 2, sharey=False)\n",
    "f.suptitle(f'User behavior based on winter hour change')\n",
    "f.set_figwidth(12)\n",
    "f.set_figheight(6)\n",
    "before_display.plot.bar(ax=arr[0], x='Hour', y='before', color='blue')\n",
    "after_display.plot(ax=arr[0], x='Hour', y='after',title='Posts', color='red')\n",
    "arr[1].plot(before.Hour,before.PostsCount-after.PostsCount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, we again see a shift of the peak of the distribution towards the right, from 7 pm to 8 pm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimation of Person correlation (based on data before and after an event analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_series = before_df.PostsCount.values\n",
    "after_series = after_df.PostsCount.values\n",
    "\n",
    "before_mean = before_df.PostsCount.mean()\n",
    "after_mean = after_df.PostsCount.mean()\n",
    "\n",
    "print(repr(sp.stats.pearsonr(before_series,after_series)))\n",
    "print(repr(sp.stats.spearmanr(before_series,after_series)))\n",
    "print(\"Quantity of changement: \" + repr(before_mean/after_mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shifted_after = after_series[1:]\n",
    "compared_before = before_series[:-1]\n",
    "before_mean = compared_before.mean()\n",
    "after_mean = shifted_after.mean()\n",
    "\n",
    "print(repr(sp.stats.pearsonr(compared_before,shifted_after)))\n",
    "print(repr(sp.stats.spearmanr(compared_before,shifted_after)))\n",
    "print(\"Quantity of changement: \" + repr(before_mean/after_mean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation increase when data shift is reduced & quantity of changement decreased. Two things can be confirmed: the users have \"working habits\" with more active time slots than others and that the change of time causes a shift of these habits of plus or minus an hour during the week following the time changement. Indeed, after having visually observed that the distributions before and after the time change are similar to an hour, we checked this with mathematics by comparing the correlations and quantities of change before and after the elimination of the hour shift. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graphically, we can say \"The afternoon activity is less afect by the hour changement than the morning\". we'll try to do a similar analysis to prove it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after_df.drop(after_df.tail(3).index,inplace=True)\n",
    "after_df = after_df.reset_index()\n",
    "before_df = before_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Morning/afternoon\n",
    "morning_before= before_df[before_df.Hour < str(12)].PostsCount\n",
    "morning_after = after_df[after_df.Hour < str(12)].PostsCount\n",
    "afternoon_before= before_df[before_df.Hour >= str(12)].PostsCount\n",
    "afternoon_after = after_df[after_df.Hour >= str(12)].PostsCount\n",
    "#morning_before=pd.concat([pd.Series([np.nan]), morning_before])\n",
    "#d = {'MorningBefore': morning_before.values, 'AfternoonBefore': afternoon_before.values,'MorningAfter':morning_after.values,'AfternoonAfter':afternoon_after.values}\n",
    "#time_df = pd.DataFrame(data=d)\n",
    "#time_df.head()\n",
    "\n",
    "f, arr = plt.subplots(1, 2, sharey=True)\n",
    "f.suptitle(f'User behavior based on winter hour change')\n",
    "f.set_figwidth(12)\n",
    "f.set_figheight(6)\n",
    "morning_before.plot(ax=arr[0], x='Hour', y='PostsCount',title='Before', color='blue',label=\"before\")\n",
    "morning_after.plot(ax=arr[0], x='Hour', y='PostsCount',title='After', color='red',label=\"after\")\n",
    "\n",
    "afternoon_after.plot(ax=arr[1], x='Hour', y='PostsCount',title='After', color='red',label=\"after\")\n",
    "afternoon_before.plot(ax=arr[1], x='Hour', y='PostsCount',title='Before', color='blue',label=\"before\")\n",
    "plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "morning_before_mean = morning_before.values.mean()\n",
    "morning_after_mean = morning_after.values.mean()\n",
    "\n",
    "print(repr(sp.stats.pearsonr(morning_before.values,morning_after.values)))\n",
    "print(repr(sp.stats.spearmanr(morning_before.values,morning_after.values)))\n",
    "print(\"Quantity of changement: \" + repr(morning_before_mean/morning_after_mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "afternoon_before_mean = afternoon_before.values.mean()\n",
    "afternoon_after_mean = afternoon_after.values.mean()\n",
    "\n",
    "print(repr(sp.stats.pearsonr(afternoon_before.values,afternoon_after.values)))\n",
    "print(repr(sp.stats.spearmanr(afternoon_before.values,afternoon_after.values)))\n",
    "print(\"Quantity of changement: \" + repr(afternoon_before_mean/afternoon_after_mean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see with our indicators, the correlation between afternoon before and after hour changement is high and the quantity of changement is close to 1, so there's no changement. For the morning, there's more divergence as we expected.\n",
    "Now, we'll try to determinated how many days need people to absorb this shift. We'll analyse days after the hour changement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#before_df = posts_date_df[:167]\n",
    "before = before_df.groupby('Hour').mean().reset_index()\n",
    "after_df = posts_date_df[167:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days = [\"Monday\",\"Tuesday\",\"Wednesday\",\"Thursday\",\"Friday\",\"Saturday\",\"Sunday\"]\n",
    "correlation = []\n",
    "f, arr = plt.subplots(7, 1, sharey=True)\n",
    "f.suptitle(f'Change counts before and after winter hour change 2018')\n",
    "f.set_figwidth(12)\n",
    "f.set_figheight(50)\n",
    "for i in range(7):\n",
    "    #before = before_df.iloc[i*24:i*24+24]\n",
    "    after = after_df.iloc[i*24:i*24+24]\n",
    "    pearson,pv = sp.stats.pearsonr(before.PostsCount.values,after.PostsCount.values)\n",
    "    spearman,pv = sp.stats.spearmanr(before.PostsCount.values,after.PostsCount.values)\n",
    "    correlation.append((pearson,spearman))\n",
    "    pt1=before.plot(ax=arr[i], x='Hour', y='PostsCount', color='red',label=\"before\")\n",
    "    pt2=after.reset_index().plot.bar(ax=arr[i], x='Hour', y='PostsCount', color='blue',label=\"after\")\n",
    "    pt2.annotate(days[i]+\": Person Correlation: \"+repr(pearson)+\" Spearman Correlation: \"+repr(spearman), (0,0), (0, -40), xycoords='axes fraction', textcoords='offset points', va='top')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(days,correlation)\n",
    "plt.title(\"Days Activity evolution\")\n",
    "_ = plt.legend([\"Pearson\",\"Spearman\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DST Impact on the way people work out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "The dataset consists of 26,000 people counts (about every 10 minutes) over the last year. In addition, it include extra info including weather and semester-specific information that might affect how crowded it is.\n",
    "\n",
    "Features:\n",
    "* __date__ (string; datetime of data)\n",
    "* __timestamp__ (int; number of seconds since beginning of day)\n",
    "* __day_of_week__ (int; 0 [monday] - 6 [sunday])\n",
    "* __is_weekend__ (int; 0 or 1) [boolean, if 1, it's either saturday or sunday, otherwise 0]\n",
    "* __is_holiday__ (int; 0 or 1) [boolean, if 1 it's a federal holiday, 0 otherwise]\n",
    "* __temperature__ (float; degrees fahrenheit)\n",
    "* __is_start_of_semester__ (int; 0 or 1) [boolean, if 1 it's the beginning of a school semester, 0 otherwise]\n",
    "* __month__ (int; 1 [jan] - 12 [dec])\n",
    "* __hour__ (int; 0 - 23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = 'data'\n",
    "traffic_data = pd.read_csv(os.path.join(DATA_DIR, 'data.csv'))\n",
    "traffic_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(traffic_data['date'].describe())\n",
    "print(\"\\nFirst date: \" + traffic_data['date'].min())\n",
    "print(\"Last date: \" + traffic_data['date'].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Changes in Los Angeles Over the Years\n",
    "#### Daylight Saving Time (DST) changes do not necessarily occur on the same date every year.\n",
    "\n",
    "| Year | Date & Time | Abbreviation | Time Change | Offset After |\n",
    "|------|-------------|--------------|-------------|--------------|\n",
    "| 2015 | Sun, 8 Mar, 02:00  | PST → PDT | +1 hour (DST start) | UTC-7h |\n",
    "| 2015 | Sun, 1 Nov, 02:00  | PDT → PST | -1 hour (DST end)   | UTC-8h |\n",
    "| 2016 | Sun, 13 Mar, 02:00 | PST → PDT\t| +1 hour (DST start) | UTC-7h |\n",
    "| 2016 | Sun, 6 Nov, 02:00  | PDT → PST | -1 hour (DST end)   | UTC-8h |\n",
    "| 2017 | Sun, 12 Mar, 02:00 | PST → PDT | +1 hour (DST start) | UTC-7h |\n",
    "| 2017 | Sun, 5 Nov, 02:00  | PDT → PST | -1 hour (DST end)   | UTC-8h |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like we have the all of the DST changes except the first and the last one. Let's split our data set to reflect these disjoint periods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will convert the date to UTC (for convenience, later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_data['date_utc'] = pd.to_datetime(traffic_data['date'], infer_datetime_format=True)\n",
    "traffic_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, since we see that the data contains information about the timezone, and the timezone changes as part of the DST change, we can take advantage of this. Basically we split our data set based on continous sequences that have the same timezone offset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = np.diff(traffic_data['date'].str.contains(\"-07:00\"))\n",
    "ind_split = np.where(diff == True)[0]\n",
    "traffic_periods = [\n",
    "    traffic_data.iloc[:ind_split[0]],\n",
    "    traffic_data.iloc[ind_split[0]:ind_split[1]],\n",
    "    traffic_data.iloc[ind_split[1]:ind_split[2]],\n",
    "    traffic_data.iloc[ind_split[2]:ind_split[3]],\n",
    "    traffic_data.iloc[ind_split[3]:],\n",
    "]\n",
    "period_names = [\n",
    "    \"2015 DST start\",\n",
    "    \"2015 DST end\",\n",
    "    \"2016 DST start\",\n",
    "    \"2016 DST end\",\n",
    "    \"2017 DST start\",\n",
    "    \"2017 DST end\"\n",
    "]\n",
    "# Check if they have the same size\n",
    "assert(len(traffic_periods[0]) + len(traffic_periods[1]) + len(traffic_periods[2]) + len(traffic_periods[3]) + len(traffic_periods[4]) == len(traffic_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try to see if we find something interesting in the data on each period. We will start by checking basic statictics about the number of people in each traffic period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, traffic_period in enumerate(traffic_periods):\n",
    "    print(f\"Period {index}:\\n\\n{traffic_period['number_people'].describe()}\")\n",
    "    print(f\"\\nTotal count: \\t{traffic_period['number_people'].sum()}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also let's note the periods we have so far, and their corresponding months\n",
    "\n",
    "|Period #| Month spans | Data Points |\n",
    "|--------|-------------|-------------|\n",
    "|0       |Aug'15-Nov'15|7637         |\n",
    "|1       |Nov'15-Mar'16|12963        |\n",
    "|2       |Mar'16-Nov'16|30571        |\n",
    "|3       |Nov'16-Mar'17|10347        |\n",
    "|4       |Mar'17-Mar'17|666          |\n",
    "|_       |Total        |62184        |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this table, we can see that the last period contains a very small amount of collected data. Indeed, we have data collected only one week after the DST change.\n",
    "\n",
    "Another note that we have to make is that for the first period we have only 4 months of data from the whole 9 months of the 2014-2015 DST period.\n",
    "\n",
    "Let's plot the total number of gym 'visits' (**workouts**) in those periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_count = []\n",
    "\n",
    "for index, traffic_period in enumerate(traffic_periods):\n",
    "    total_count.append(traffic_period['number_people'].sum())\n",
    "print(total_count)\n",
    "plt.bar(x = range(len(total_count)), height = total_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ignoring the first and the last period, based on the above notes, an interesting observation we can make from this plot is that in the Nov'15-Mar'16 period, the total number of workout sessions was more than twice that number in the same period of the next year (Nov'16-Mar'17). Said differently, the number of session workouts dropped by more than half one year after."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monthly statistics\n",
    "We will increase the granularity of our analysis on a monthly basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_monthly_stats(traffic_period, index):\n",
    "    #display(traffic_period.groupby('month')['number_people'].agg('sum'))\n",
    "    traffic_period.groupby('month')['number_people'].agg('sum').plot.bar(color='blue', alpha=0.5)\n",
    "    plt.title(f\"Period {index}\")\n",
    "    plt.show()\n",
    "    #traffic_period.hist(column = 'number_people', by='month')\n",
    "for index, traffic_period in enumerate(traffic_periods):\n",
    "    plot_monthly_stats(traffic_period, index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One month before and after DST\n",
    "Let's now focus our analysis only on the month before and after the hour change, in order to look for more interesting pattern changes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(len(traffic_periods) - 1):\n",
    "    period_before = traffic_periods[index]\n",
    "    period_after = traffic_periods[index + 1]\n",
    "    one_month_before_dst = period_before[period_before['date_utc'] >= (period_before['date_utc'].max() - timedelta(30))]\n",
    "    one_month_after_dst = period_after[period_after['date_utc'] <= (period_after['date_utc'].min() + timedelta(30))]\n",
    "    print(f\"Time change #{index}\")\n",
    "    print(f\"Data points one month before: {len(one_month_before_dst)}\")\n",
    "    print(f\"Data points one month after: {len(one_month_after_dst)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will ignore the last change, becaue we only have 8 days available on the period after the March 2017 time change, and not enough data points.\n",
    "In order to visualise this data, we could plot the distrubtion of the number of people on every day of the week. That makes sense from the perspective of our data domain (since most of the workouts programs are tailored on a week by week basis - ie Monday - Leg day, Friday - Arms day 💪)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, len(traffic_periods) - 2, figsize = (15, 4))\n",
    "fig.tight_layout()\n",
    "for index in range(len(traffic_periods) - 2):\n",
    "    period_before = traffic_periods[index]\n",
    "    period_after = traffic_periods[index + 1]\n",
    "    one_month_before_dst = period_before[period_before['date_utc'] >= (period_before['date_utc'].max() - timedelta(30))]\n",
    "    one_month_after_dst = period_after[period_after['date_utc'] <= (period_after['date_utc'].min() + timedelta(30))]\n",
    "    one_month_before_dst.groupby('day_of_week')['number_people'].agg('sum').plot.bar(color='blue', alpha=0.5, ax = ax[index])\n",
    "    one_month_after_dst.groupby('day_of_week')['number_people'].agg('sum').plot.bar(color='red', alpha=0.5, ax = ax[index])\n",
    "    ax[index].set_title(f\"{period_names[index]}\")\n",
    "    ax[index].set_xticklabels([\"Monday\", \"Tuesday\", \"Wednesday\", \"Tuesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"])\n",
    "    ax[index].set_xlabel(\"Day of week\")\n",
    "ax[0].set_ylabel(\"Total count\")\n",
    "handles, labels = ax[-1].get_legend_handles_labels()\n",
    "fig.legend(handles, ['Total before change', 'Total after change'], loc='lower center')\n",
    "fig.savefig('total_before_after.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the results are pretty consistent. The distributions skews to the left, perhaps because people are thinking about going to the gym as early as possible (the peak is not Monday, not Thursday). We can think of this as the \"New Year's Resolution\" effect, when people are very motivated to start going to the gym and eating healthy right after the New Year's Eve, but they loose momentun in the following months, perhaps returning to normal, as soon as the NY vibe is gone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heatmaps\n",
    "\n",
    "Let's now try to plot a heatmap for each day of the week and for each hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heatmap(df, index, ax, after = False, cbar = False):\n",
    "    myMap = np.zeros(shape = [7, 24])\n",
    "    for row in df.iterrows():\n",
    "        myMap[row[1]['day_of_week'], row[1]['hour']] += row[1]['number_people']\n",
    "    sns.heatmap(myMap, square=True, ax = ax, cbar = cbar)\n",
    "    plt.tight_layout()\n",
    "    if after:\n",
    "        ax.set_title(f\"Crowdness at the gym in the period after {period_names[index]}\")\n",
    "    else:\n",
    "        ax.set_title(f\"Crowdness at the gym in the period before {period_names[index]}\")\n",
    "    ax.set_xlabel(\"Day of week\")\n",
    "    plt.setp(ax.xaxis.get_majorticklabels(), rotation=0)\n",
    "    ax.set_yticklabels([\"Monday\", \"Tuesday\", \"Wednesday\", \"Tuesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"])\n",
    "    plt.setp(ax.yaxis.get_majorticklabels(), rotation=0)\n",
    "\n",
    "fig, ax = plt.subplots(len(traffic_periods) - 2, 2, figsize = (15, 7))\n",
    "for index in range(len(traffic_periods) - 2):\n",
    "    period_before = traffic_periods[index]\n",
    "    period_after = traffic_periods[index + 1]\n",
    "    one_month_before_dst = period_before[period_before['date_utc'] >= (period_before['date_utc'].max() - timedelta(30))]\n",
    "    one_month_after_dst = period_after[period_after['date_utc'] <= (period_after['date_utc'].min() + timedelta(30))]\n",
    "    plot_heatmap(one_month_before_dst, index, ax[index, 0], cbar = True)\n",
    "    plot_heatmap(one_month_after_dst, index, ax[index, 1], after = True, cbar = True)\n",
    "fig.savefig(\"heatmap_crowndess.png\", bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen from these heatmaps that the number of coming early in the morning (6am) increases after the DST - which makes sense, and we were expecting that.\n",
    "\n",
    "__A very interesting pattern that we notice is that on every change, the highest traffic at the gym before the change was always on Thursdays (white squares) afternoon, between 14-20, and the cluster after was always on Mondays on pretty much the same period.__\n",
    "\n",
    "Let's now aggregate the days based on the hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1, len(traffic_periods) - 2, figsize = (20, 4))\n",
    "for index in range(len(traffic_periods) - 2):\n",
    "    period_before = traffic_periods[index]\n",
    "    period_after = traffic_periods[index + 1]\n",
    "    one_month_before_dst = period_before[period_before['date_utc'] >= (period_before['date_utc'].max() - timedelta(30))]\n",
    "    one_month_after_dst = period_after[period_after['date_utc'] <= (period_after['date_utc'].min() + timedelta(30))]\n",
    "    one_month_before_dst.groupby('hour')['number_people'].mean().plot.bar(color='blue', alpha=0.5, ax = ax[index])\n",
    "    one_month_after_dst.groupby('hour')['number_people'].mean().plot.bar(color='red', alpha=0.5, ax = ax[index])\n",
    "    ax[index].set_title(f\"Period {index}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the distribution of the mean number of people on each our is decreasing with some very very rare cases. One explication can be that most of the people's [sleep is affected by the irregularity](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2718885/). Indeed, used to sleep $X$ number of hours and then one day sleeping $X + 1$ hours might actually perturb our sleep, and people might try to overcome this fatigue by something different than sport. Another potetial explanation can be given by the fact that since the gym is in the UC Berkely campus, the students and the mid-term exams period starts in November, students use DST as a 'breaking point' to stop going to the gym for a while, and study more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's further see the correlations of the features before and after the changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_correlation(df, ax = None, title = 'Correlation between different features'):\n",
    "    correlation = df.corr()[:1]\n",
    "    plt.figure(figsize=(10,10))\n",
    "    if ax:\n",
    "        sns.heatmap(correlation, vmax=1, square=True, annot=True, cmap='viridis', ax = ax)\n",
    "    else:\n",
    "        ax = sns.heatmap(correlation, vmax=1, square=True, annot=True, cmap='viridis')\n",
    "\n",
    "    ax.set_title(title)\n",
    "show_correlation(traffic_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(1):#len(traffic_periods) - ):\n",
    "    period_before = traffic_periods[index]\n",
    "    period_after = traffic_periods[index + 1]\n",
    "    period_before = period_before.drop(columns = ['is_holiday'])\n",
    "    period_after = period_after.drop(columns = ['is_holiday'])\n",
    "    corr = pd.concat([period_before.corr()[:1], period_after.corr()[:1]], axis=0)\n",
    "    sns.heatmap(corr, vmax=1, square=True, annot=True, cmap='viridis')\n",
    "plt.savefig(\"correlation.png\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the correlation before (the top row) and after (the bottom row) have changed. First, the hour seems not to be much correlated, but the temperature does seem to be more correlated. As stated earlier, the fact that the **is_start_semester** is very correlated with the number of peple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
